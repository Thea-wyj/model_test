{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01f8ef5-c386-41c6-b7e0-4c1bc430ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import os\n",
    "\n",
    "import foolbox as fa\n",
    "import model\n",
    "import torch\n",
    "import torchvision\n",
    "from data import loader\n",
    "from data import preprocess\n",
    "from foolbox import PyTorchModel, accuracy\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr  # 计算信噪比\n",
    "from utils import common\n",
    "from skimage.metrics import structural_similarity as ssim  # 计算图片平均结构相似度\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5195b9-7335-490d-9098-be4a07a1d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, is_train, is_shuffle, name, batch_size):\n",
    "    _dataset = loader.Dataset(\n",
    "        path, name=name, is_train=is_train,\n",
    "        transform=torchvision.transforms.Compose([\n",
    "            preprocess.CenterCrop(88), \n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        _dataset, batch_size=batch_size, shuffle=is_shuffle, num_workers=1\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be596b1a-75f7-4c49-9500-b7d1b8264f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(_m, ds):\n",
    "    num_data = 0\n",
    "    corrects = 0\n",
    "\n",
    "    _m.net.eval()\n",
    "    _softmax = torch.nn.Softmax(dim=1)\n",
    "    for i, data in enumerate(ds):\n",
    "        images, labels, _ = data\n",
    "        predictions = _m.inference(images)\n",
    "        predictions = _softmax(predictions)\n",
    "\n",
    "        _, predictions = torch.max(predictions.data, 1)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        num_data += labels.size(0)\n",
    "        corrects += (predictions == labels.to(m.device)).sum().item()\n",
    "\n",
    "    accuracy = 100 * corrects / num_data\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ee5bb-27b6-4b30-b8d5-838cd20d7d09",
   "metadata": {},
   "source": [
    "### 加载模型和数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08b261ad-50d8-46fc-933e-7656a02aaa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load test data set: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load test data set: 2425it [00:01, 2376.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (_layer): Sequential(\n",
       "    (0): Conv2DBlock(\n",
       "      (_layer): Sequential(\n",
       "        (conv): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1), padding=valid)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv2DBlock(\n",
       "      (_layer): Sequential(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=valid)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Conv2DBlock(\n",
       "      (_layer): Sequential(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(6, 6), stride=(1, 1), padding=valid)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Conv2DBlock(\n",
       "      (_layer): Sequential(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=valid)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): Conv2DBlock(\n",
       "      (_layer): Sequential(\n",
       "        (conv): Conv2d(128, 10, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "      )\n",
       "    )\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = common.load_config(os.path.join(common.project_root, 'experiments/config/AConvNet-SOC.json'))\n",
    "model_name = config['model_name']\n",
    "\n",
    "\n",
    "m = model.Model(\n",
    "    classes=config['num_classes'], channels=config['channels'],\n",
    ")\n",
    "\n",
    "best_path = '/guoxuan/AConvNet-pytorch-main/experiments/model/AConvNet-SOC/model-030.pth'\n",
    "test_set = load_dataset('dataset', False, True, 'soc', 100)\n",
    "\n",
    "m.load(best_path)\n",
    "m.net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feaf890b-782d-4b23-91dd-1d4c1c128fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'model.network.Network'> <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(m.net), type(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72ccaaa2-7bb3-4b11-818a-52dbc52f8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "# from torch.autograd.gradcheck import zero_gradients\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af66dae-f7ff-49cd-bf98-170b3048267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_gradients(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        if x.grad is not None:\n",
    "            x.grad.detach_()\n",
    "            x.grad.zero_()\n",
    "    elif isinstance(x, collections.abc.Iterable):\n",
    "        for elem in x:\n",
    "            zero_gradients(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d022955a-e855-4f41-9809-05e67015f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saliency map\n",
    "# 此方法为beta参数的简化版本，注重攻击目标贡献大的点\n",
    "def saliency_map(F,x,t,mask):\n",
    "    # F 为模型的输出\n",
    "    # t 为攻击的类别\n",
    "    # x 表示输入的图像\n",
    "    # mask 标记位，记录已经访问的点的坐标\n",
    "    F[0,t].backward(retain_graph=True)\n",
    "    derivative=x.grad.data.cpu().numpy().copy()\n",
    "    alphas=derivative*mask # 预测 对攻击目标的贡献\n",
    "    betas=-np.ones_like(alphas) # 预测对非攻击目标的贡献\n",
    "    sal_map=np.abs(alphas)*np.abs(betas)*np.sign(alphas*betas)\n",
    "    idx=np.argmin(sal_map) # 最佳像素和扰动方向\n",
    "    idx=np.unravel_index(idx,mask.shape) # 转换成(p1,p2)格式\n",
    "    pix_sign=np.sign(alphas)[idx]\n",
    "    return idx,pix_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f41f87a-5061-4789-b11e-c69fc0a8082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(test_set):\n",
    "    images, labels, _ = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2366059-a5f8-463e-891b-b3b78b3b8631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 2, 88, 88]), torch.Size([100]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0fa15fb-b629-4622-b1fe-35f2dde927af",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25247b2e-f869-4d07-b344-98c6c51c62af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 88, 88])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38bc6e88-9876-4c98-a900-5b0cc90c52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.to(m.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82ae557e-40e7-40d0-902a-0f2425b17aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c1c2914-5dd7-4d89-b972-be66a7298614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(img), type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "587edd2c-dd3e-45de-abfe-93dc0f1d8a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 88, 88]), torch.Size([]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d39e7f0-d117-447c-9782-64f2a59785c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 88, 88]), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, img.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffc6d929-dfa1-4adf-b4b9-305cbeaaded3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45741a31-c4b8-4374-a239-655c68f620e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c001cfd-5ae6-468e-b068-c86b4f0c4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = m.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd36550f-e046-4116-9b45-4877a231da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = m.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "440b488c-ef9d-4448-b8c1-2bc1986d0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2de9d86a-acf1-4078-a797-b33889a5e272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 9.3760,  5.8173,  0.2222, -3.5017,  0.8242, -4.7103, -1.7983,  0.9111,\n",
      "         -0.6978, -3.6384]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=0 label=0 loss=10.102455139160156\n",
      "idx =  (0, 0, 22, 47) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 9.2362,  5.6072,  0.3953, -3.6793,  0.5434, -4.4768, -1.8852,  0.7297,\n",
      "         -0.1960, -3.5010]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=1 label=0 loss=9.45899772644043\n",
      "idx =  (0, 0, 22, 47) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 9.0884,  5.3819,  0.5744, -3.8725,  0.2508, -4.2229, -2.0042,  0.5473,\n",
      "          0.3537, -3.3579]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=2 label=0 loss=8.75967788696289\n",
      "idx =  (0, 0, 22, 47) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.9452,  5.1620,  0.7720, -4.0705, -0.0434, -3.9811, -2.1387,  0.3647,\n",
      "          0.9083, -3.2208]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=3 label=0 loss=8.060349464416504\n",
      "idx =  (0, 0, 22, 47) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.8219,  4.9570,  0.9526, -4.2457, -0.3000, -3.7664, -2.2662,  0.1884,\n",
      "          1.4124, -3.0921]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=4 label=0 loss=7.4315185546875\n",
      "idx =  (0, 0, 22, 47) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.7168,  4.7581,  1.1172, -4.4124, -0.5354, -3.5819, -2.3823,  0.0252,\n",
      "          1.8891, -2.9697]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=5 label=0 loss=6.848426342010498\n",
      "idx =  (0, 0, 22, 46) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.6187,  4.5545,  1.2542, -4.5762, -0.7858, -3.4153, -2.4697, -0.1366,\n",
      "          2.3661, -2.8347]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=6 label=0 loss=6.272477149963379\n",
      "idx =  (0, 0, 22, 46) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.5096,  4.3449,  1.3737, -4.6975, -0.9931, -3.2803, -2.5342, -0.2750,\n",
      "          2.7894, -2.7041]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=7 label=0 loss=5.739930152893066\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.4034,  4.1270,  1.5280, -4.8353, -1.1925, -3.1478, -2.6048, -0.4233,\n",
      "          3.2116, -2.5777]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=8 label=0 loss=5.212337493896484\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.3100,  3.9323,  1.6660, -4.9465, -1.3783, -3.0500, -2.6532, -0.5361,\n",
      "          3.5810, -2.4705]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=9 label=0 loss=4.751693248748779\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.2389,  3.7408,  1.8034, -5.0585, -1.5426, -2.9816, -2.7055, -0.6513,\n",
      "          3.9347, -2.3583]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=10 label=0 loss=4.330393314361572\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.1664,  3.5441,  1.9374, -5.1635, -1.7000, -2.9172, -2.7529, -0.7652,\n",
      "          4.2773, -2.2435]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=11 label=0 loss=3.921144962310791\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.0986,  3.3502,  2.0623, -5.2834, -1.8561, -2.8592, -2.7919, -0.8745,\n",
      "          4.6247, -2.1229]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=12 label=0 loss=3.5152950286865234\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 8.0386,  3.1535,  2.1844, -5.4114, -2.0175, -2.8077, -2.8257, -0.9798,\n",
      "          4.9829, -1.9966]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=13 label=0 loss=3.1117889881134033\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 7.9799,  2.9515,  2.3054, -5.5441, -2.1808, -2.7568, -2.8554, -1.0840,\n",
      "          5.3459, -1.8659]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=14 label=0 loss=2.7127816677093506\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 7.9235,  2.7504,  2.4283, -5.6731, -2.3427, -2.7182, -2.8827, -1.1809,\n",
      "          5.7043, -1.7387]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=15 label=0 loss=2.331399440765381\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 7.8704,  2.5574,  2.5459, -5.7973, -2.4792, -2.7048, -2.9051, -1.2612,\n",
      "          6.0359, -1.6227]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=16 label=0 loss=1.9913105964660645\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 7.8432,  2.4008,  2.6373, -5.8786, -2.5653, -2.7312, -2.9119, -1.3159,\n",
      "          6.2933, -1.5503]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=17 label=0 loss=1.7506884336471558\n",
      "idx =  (0, 0, 21, 43) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 7.7938,  2.2500,  2.6883, -5.9400, -2.7069, -2.6750, -2.9128, -1.3925,\n",
      "          6.5449, -1.4467]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=18 label=0 loss=1.509000301361084\n",
      "idx =  (0, 0, 21, 43) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 7.7392,  2.0972,  2.7370, -5.9876, -2.8421, -2.6203, -2.9101, -1.4676,\n",
      "          6.7819, -1.3444]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=19 label=0 loss=1.28981614112854\n",
      "idx =  (0, 0, 19, 45) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 7.6705,  1.9186,  2.8293, -6.0010, -2.9933, -2.5548, -2.9391, -1.5583,\n",
      "          7.0070, -1.2364]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=20 label=0 loss=1.0864267349243164\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 7.6350,  1.7687,  2.9049, -6.0500, -3.0608, -2.5935, -2.9374, -1.6062,\n",
      "          7.2254, -1.1615]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=21 label=0 loss=0.9259273409843445\n",
      "idx =  (0, 0, 17, 44) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 7.5681,  1.5905,  3.0009, -6.0540, -3.2226, -2.4947, -2.9755, -1.7069,\n",
      "          7.4304, -1.0572]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=22 label=0 loss=0.7714318037033081\n",
      "idx =  (0, 0, 22, 48) pix_sign =  1.0\n",
      "torch.Size([1]) tensor([8], device='cuda:0') torch.Size([1, 10]) tensor([[ 7.5460,  1.4692,  3.0633, -6.1017, -3.2663, -2.5496, -2.9632, -1.7435,\n",
      "          7.6160, -1.0002]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "epoch=23 label=8 loss=0.6654925346374512\n"
     ]
    }
   ],
   "source": [
    "epochs=500\n",
    "theta=0.01 # 扰动系数\n",
    "target_label=8 #攻击目标\n",
    "target=Variable(torch.Tensor([float(target_label)]).to(device).long())\n",
    "loss_func=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mask=np.ones_like(img.data.cpu().numpy()) # 定义搜索域，修改后的位置置零\n",
    "# 定义边界\n",
    "max_=16.7701\n",
    "min_=0\n",
    "for epoch in range(epochs):\n",
    "    output=net(img)\n",
    "    label=np.argmax(output.data.cpu().numpy())\n",
    "    print(target.shape, target, output.shape, output)\n",
    "    loss=loss_func(output,target)\n",
    "    print('epoch={} label={} loss={}'.format(epoch,label,loss))\n",
    "    if label==target_label:\n",
    "        break  # 攻击成功\n",
    "    zero_gradients(img) # 梯度清零\n",
    "    idx,pix_sign=saliency_map(output,img,target_label,mask)\n",
    "    print(\"idx = \", idx, \"pix_sign = \", pix_sign)\n",
    "    # 添加扰动\n",
    "    img.data[idx]=img.data[idx]+pix_sign*theta*(max_ - min_)\n",
    "    # 达到极限的点不再参与更新\n",
    "    if(img.data[idx] <= min_)or(img.data[idx] >= max_):\n",
    "        print('idx={} over {}'.format(idx,img.data[idx]))\n",
    "        mask[idx]=0\n",
    "        img.data.cpu()[idx]=np.clip(img.data.cpu()[idx],min_,max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "145b9496-b52c-4228-b520-344c873ae16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 88, 88]), True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, img.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "92a99de6-a571-496b-b114-3a2b6cb68401",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.requries_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a89a2ca2-9956-4b36-8c6a-a9655efc604f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2, 88, 88])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3abdffb-9470-4973-8347-9ac16958c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_img = images[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e44ed53-05d0-4aa4-b100-65ece0225da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_img = ori_img.to(m.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "417819fa-80e7-4d69-9d43-071fca5711d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02eef0ab-8060-4b37-a87a-1561e9a0bf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 88, 88])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513fbd02-a1ea-4b7c-bf13-52c917e7439d",
   "metadata": {},
   "source": [
    "## 计算指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25706617-7f69-4989-bb82-0bc7f07b05f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 88, 88]) torch.Size([1, 2, 88, 88])\n"
     ]
    }
   ],
   "source": [
    "print(ori_img.shape, img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07acc942-2abd-40ad-8549-5da9bf51ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算相关的指标\n",
    "def calculate_avg_norm_distortion_factor(select_raw_images, select_adv_images, data_range=12):\n",
    "    \"\"\"\n",
    "        计算相应的指标\n",
    "    \"\"\"\n",
    "\n",
    "    # 没有对抗样本\n",
    "    if select_raw_images.shape[0] == 0:\n",
    "        return [0, 0, 0], 0, 0, [0, 0, 0]\n",
    "        \n",
    "    linf_avg_epsion = torch.norm(select_raw_images - select_adv_images, p=float('inf'), dim=(1, 2, 3))\n",
    "    l1_avg_epsion = torch.norm(select_raw_images - select_adv_images, p=1, dim=(1, 2, 3))\n",
    "    l2_avg_epsion = torch.norm(select_raw_images - select_adv_images, p=2, dim=(1, 2, 3))\n",
    "\n",
    "    linf_distortion_factor = (linf_avg_epsion\n",
    "                              / torch.norm(select_raw_images, p=float('inf'), dim=(1, 2, 3))).mean().item()\n",
    "    l1_distortion_factor = (l1_avg_epsion\n",
    "                            / torch.norm(select_raw_images, p=1, dim=(1, 2, 3))).mean().item()\n",
    "    l2_distortion_factor = (l2_avg_epsion\n",
    "                            / torch.norm(select_raw_images, p=2, dim=(1, 2, 3))).mean().item()\n",
    "\n",
    "    psnr_item = psnr(select_raw_images.cpu().numpy(), select_adv_images.cpu().numpy(), data_range=data_range)\n",
    "\n",
    "    #ssim_item = ssim(select_raw_images.permute(0, 2, 3, 1).cpu().numpy(), select_adv_images.permute(0, 2, 3, 1).cpu().numpy(),\n",
    "                        # win_size=11, multichannel=True).item() # channel_last\n",
    "    metric = dict()\n",
    "    metric[\"l1_distortion_factor\"] = l1_distortion_factor\n",
    "    metric[\"l2_distortion_factor\"] = l2_distortion_factor\n",
    "    metric[\"linf_distortion_factor\"] = linf_distortion_factor\n",
    "    metric[\"psnr_item\"] = psnr_item\n",
    "    metric[\"l1_avg_epsion\"] = l1_avg_epsion.mean().item()\n",
    "    metric[\"l2_avg_epsion\"] = l2_avg_epsion.mean().item()\n",
    "    metric[\"linf_avg_epsion\"] = linf_avg_epsion.mean().item()\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58443248-73ea-46d0-a75a-24d56f69805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"l1_distortion_factor\": 0.00015901018923614174,\n",
      "    \"l2_distortion_factor\": 0.007133711129426956,\n",
      "    \"linf_distortion_factor\": 0.3203635811805725,\n",
      "    \"psnr_item\": 56.46433389246155,\n",
      "    \"l1_avg_epsion\": 3.857123374938965,\n",
      "    \"l2_avg_epsion\": 2.2436866760253906,\n",
      "    \"linf_avg_epsion\": 2.0124123096466064\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(calculate_avg_norm_distortion_factor(ori_img, img), indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c6d37e1-4f63-40ba-8d43-7eb4ab0a4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = img.data.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f39501ae-54d3-4aee-96ce-9a8fd8c4a50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 88, 88)\n"
     ]
    }
   ],
   "source": [
    "print(adv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8e7903c-94ea-4fdd-911f-ffd2a344b5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e4fd6-db92-4090-aff8-71c848d5438a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
